{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "#nlp = spacy.load('en_core_web_sm') \n",
    "\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_infix_regex\n",
    "\n",
    "nlp = spacy.load(r'C:\\\\Users\\\\Marta\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python38\\\\Lib\\\\site-packages\\\\en_core_web_sm\\\\en_core_web_sm-2.2.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUERY API: https://krr.triply.cc/annadg/-/queries/Abstract-Data-Query/1\n",
    "data = pd.read_csv('entityQueryResults.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_match(text):\n",
    "    \"\"\"function to find sentences that contain the lemma of hypothesis\"\"\"\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    # Add match ID \"HypothesisIs\" with no callback and one pattern\n",
    "    pattern = [{'LEMMA':{\"IN\":[\"hypothesis\",\"hypothesize\",\"hypothesise\", \"hypothesized\", \"hypothesised\"]}}]\n",
    "    \n",
    "    matcher.add(\"HypothesisIs\", None, pattern)\n",
    "\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "        span = doc[start:end]  # The matched span\n",
    "        sent = span.sent\n",
    "        return sent.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pattern_match'] = data['value'].apply(pattern_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to get rid of this instance, otherwise the merge noun chunks does not work. Drop this instance with the following cell. \n",
    "data[data['pattern_match']==\"While the hypothesis that dromedary camels are the likely major source of MERS-CoV infection in humans is gaining acceptance, conjecture continues over the original natural reservoir host(s)\"].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([data.index[220]], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating bool series True for NaN values - as the subsequent formula will break if there are \n",
    "bool_series = pd.isnull(data[\"pattern_match\"])  \n",
    "    \n",
    "# filtering data  \n",
    "data[bool_series]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate abstracts as not matching lemma pattern to verify integrity of pattern\n",
    "\n",
    "for row in data.value[881:882]:  # iterating through the rows of the object column\n",
    "    print(row, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'pattern match' does not return a match\n",
    "data.dropna(subset=['pattern_match'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_nps = nlp.create_pipe(\"merge_noun_chunks\")\n",
    "nlp.add_pipe(merge_nps)\n",
    "\n",
    "def merge_noun_chunks(text):\n",
    "    \"\"\"function to merge noun chunks in texts\"\"\"\n",
    "    noun_chunks = []\n",
    "    for t in nlp(text):\n",
    "        noun_chunks.append(t.text)\n",
    "        \n",
    "    return noun_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['merged_noun_chunks'] = data['pattern_match'].apply(merge_noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_chunks(list_of_chunks):\n",
    "    for index, word in enumerate(list_of_chunks):\n",
    "        if len(word.split(' ')) > 1:\n",
    "            new_word = word.replace(' ', '_')\n",
    "            list_of_chunks[index] = new_word\n",
    "    sentence = ' '.join(list_of_chunks)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['merged_sent'] = data['merged_noun_chunks'].apply(combine_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution from https://stackoverflow.com/questions/59993683/how-can-i-get-spacy-to-stop-splitting-both-hyphenated-numbers-and-words-into-sep \n",
    "def custom_tokenizer(nlp):\n",
    "    \"\"\"\n",
    "    Function that keeps intra-hyphenated words as single tokens.\n",
    "    \"\"\"\n",
    "    inf = list(nlp.Defaults.infixes)               # Default infixes\n",
    "    inf.remove(r\"(?<=[0-9])[+\\-\\*^](?=[0-9-])\")    # Remove the generic op between numbers or between a number and a -\n",
    "    inf = tuple(inf)                               # Convert inf to tuple\n",
    "    infixes = inf + tuple([r\"(?<=[0-9])[+*^](?=[0-9-])\", r\"(?<=[0-9])-(?=-)\"])  # Add the removed rule after subtracting (?<=[0-9])-(?=[0-9]) pattern\n",
    "    infixes = [x for x in infixes if '-|–|—|--|---|——|~' not in x] # Remove - between letters rule\n",
    "    infix_re = compile_infix_regex(infixes)\n",
    "\n",
    "    return Tokenizer(nlp.vocab, prefix_search=nlp.tokenizer.prefix_search,\n",
    "                                suffix_search=nlp.tokenizer.suffix_search,\n",
    "                                infix_finditer=infix_re.finditer,\n",
    "                                token_match=nlp.tokenizer.token_match,\n",
    "                                rules=nlp.Defaults.tokenizer_exceptions)\n",
    "\n",
    "nlp.tokenizer = custom_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(hypothesis_sentence):\n",
    "    hypothesis_keywords = []\n",
    "    doc = nlp(hypothesis_sentence)\n",
    "    for tok in doc:\n",
    "        if tok.pos_ == \"PROPN\" or tok.pos_ == \"NOUN\":\n",
    "            hypothesis_keywords.append(tok.text)\n",
    "   \n",
    "    return hypothesis_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['hypothesis_entities'] = data['merged_sent'].apply(get_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns = ['abstract_entities', 'merged_noun_chunks', 'merged_sent'] , inplace=True)\n",
    "data.rename(columns={\"pattern_match\":\"hypothesis_sentence\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hypothesis_entities(text):\n",
    "    cleaned_hypotheses = []\n",
    "    for word in (text):\n",
    "        word.split(' ')\n",
    "        new_word = word.replace('_', ' ')\n",
    "        cleaned_hypotheses.append(new_word)\n",
    "    \n",
    "    return cleaned_hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"clean_hypothesis_entities\"] = data[\"hypothesis_entities\"].apply(clean_hypothesis_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: 'entity_{}'.format(x + 1)\n",
    "entity_df = pd.DataFrame(data.clean_hypothesis_entities.values.tolist(),data.index, dtype=object).fillna('').rename(columns=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index(drop=True)\n",
    "entity_df = entity_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = (pd.concat([data,entity_df],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.to_csv('paper_hyp_entity_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python37364bita70c269af5a84d66b3257b9b9325a5a7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
