{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')  # loading the model, which by default has pipeline ['tagger', 'parser', 'ner']\n",
    "\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_infix_regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and create new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyps = pd.read_csv('hyps3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get hypothesis sentences and noun chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_match(text):\n",
    "    \"\"\"function to find sentences that contain the lemma of hypothesis\"\"\"\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    # Add match ID \"HypothesisIs\" with no callback and one pattern\n",
    "    pattern = [{'LEMMA':{\"IN\":[\"hypothesis\",\"hypothesize\",\"hypothesise\", \"hypothesized\", \"hypothesised\"]}}]\n",
    "    \n",
    "    matcher.add(\"HypothesisIs\", None, pattern)\n",
    "\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "        span = doc[start:end]  # The matched span\n",
    "        sent = span.sent\n",
    "        return sent.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(hyps['object'])\n",
    "df['pattern_match'] = df['object'].apply(pattern_match)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creating bool series True for NaN values - as the subsequent formula will break if there are \n",
    "# bool_series = pd.isnull(df[\"pattern_match\"])  \n",
    "    \n",
    "# # filtering data  \n",
    "# df[bool_series]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIP IF NO NaN values found\n",
    "# Take a look at the abstracts where the match was not found\n",
    "# Determine if it is a matching error or if there is in fact no match\n",
    "\n",
    "# for row in data_df.text[460:461]:  # iterating through the rows of the object column\n",
    "#     print(row, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaN rows from the dataframe (or do something smarter to filter them out so they don't break subsequent functions)\n",
    "#data_df = data_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** comment the next cell if the merged noun chunks normally works for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([df.index[455]], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_nps = nlp.create_pipe(\"merge_noun_chunks\")\n",
    "nlp.add_pipe(merge_nps)\n",
    "\n",
    "def merge_noun_chunks(text):\n",
    "    \"\"\"function to merge noun chunks in texts\"\"\"\n",
    "    noun_chunks = []\n",
    "    for t in nlp(text):\n",
    "        noun_chunks.append(t.text)\n",
    "        \n",
    "    return noun_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# was helpful for finding the sentence causing the error\n",
    "# docs = []\n",
    "# error_lst = []\n",
    "# error_text_lst = []\n",
    "# for text in df['pattern_match']: \n",
    "#     try: \n",
    "#         doc = nlp(text)\n",
    "#         docs.append(doc)\n",
    "#     except ValueError as e: \n",
    "#         error_lst.append(e)\n",
    "#         error_text_lst.append(text)\n",
    "#         print(\"Error found in sentence: \" + text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df['pattern_match']==\"While the hypothesis that dromedary camels are the likely major source of MERS-CoV infection in humans is gaining acceptance, conjecture continues over the original natural reservoir host(s)\"].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run formula on the 'pattern_match' column\n",
    "df['merged_noun_chunks'] = df['pattern_match'].apply(merge_noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_chunks(list_of_chunks):\n",
    "    for index, word in enumerate(list_of_chunks):\n",
    "        if len(word.split(' ')) > 1:\n",
    "            new_word = word.replace(' ', '_')\n",
    "            list_of_chunks[index] = new_word\n",
    "    sentence = ' '.join(list_of_chunks)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_nouns_list = data['merged_noun_chunks'])\n",
    "df['merged_sent'] = df['merged_noun_chunks'].apply(combine_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>object</th>\n",
       "      <th>pattern_match</th>\n",
       "      <th>merged_noun_chunks</th>\n",
       "      <th>merged_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MUC1 variable number tandem repeats (VNTRs) co...</td>\n",
       "      <td>Therefore, we hypothesize that a MUC1 VNTR TAC...</td>\n",
       "      <td>[Therefore, ,, we, hypothesize, that, a_MUC1_V...</td>\n",
       "      <td>Therefore , we hypothesize that a_MUC1_VNTR_TA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BACKGROUND: Mounting evidence suggests that ho...</td>\n",
       "      <td>We conducted a scoping review of the literatur...</td>\n",
       "      <td>[We, conducted, a_scoping_review, of, the_lite...</td>\n",
       "      <td>We conducted a_scoping_review of the_literatur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abstract Enterohemorrhagic Escherichia coli (E...</td>\n",
       "      <td>Thus, we hypothesize that the expression of si...</td>\n",
       "      <td>[Thus, ,, we, hypothesize, that, the_expressio...</td>\n",
       "      <td>Thus , we hypothesize that the_expression of s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The digestive tract is the entry site for tran...</td>\n",
       "      <td>Therefore, we hypothesized that RA could induc...</td>\n",
       "      <td>[Therefore, ,, we, hypothesized, that, RA, cou...</td>\n",
       "      <td>Therefore , we hypothesized that RA could indu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BACKGROUND: FIV infection frequently compromis...</td>\n",
       "      <td>We hypothesized that FIV infection may cause d...</td>\n",
       "      <td>[We, hypothesized, that, FIV_infection, may, c...</td>\n",
       "      <td>We hypothesized that FIV_infection may cause d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>Abstract P/V gene substitutions convert the no...</td>\n",
       "      <td>Here, we used two distinct animal model system...</td>\n",
       "      <td>[Here, ,, we, used, two_distinct_animal_model_...</td>\n",
       "      <td>Here , we used two_distinct_animal_model_syste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>Alcoholic liver disease (ALD) is characterized...</td>\n",
       "      <td>We hypothesized that TREM‐1 signaling contribu...</td>\n",
       "      <td>[We, hypothesized, that, TREM‐1, signaling, co...</td>\n",
       "      <td>We hypothesized that TREM‐1 signaling contribu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>The unique ornamental features and extreme sex...</td>\n",
       "      <td>Innate and adaptive immune genes involved in c...</td>\n",
       "      <td>[Innate_and_adaptive_immune_genes, involved, i...</td>\n",
       "      <td>Innate_and_adaptive_immune_genes involved in c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>In the recent years, it has been demonstrated ...</td>\n",
       "      <td>Based on an extensive bibliography where the i...</td>\n",
       "      <td>[Based, on, an_extensive_bibliography, where, ...</td>\n",
       "      <td>Based on an_extensive_bibliography where the_i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>We provide experimental evidence of a replicat...</td>\n",
       "      <td>We hypothesize that this modulatory role may b...</td>\n",
       "      <td>[We, hypothesize, that, this_modulatory_role, ...</td>\n",
       "      <td>We hypothesize that this_modulatory_role may b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>551 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                object  \\\n",
       "0    MUC1 variable number tandem repeats (VNTRs) co...   \n",
       "1    BACKGROUND: Mounting evidence suggests that ho...   \n",
       "2    Abstract Enterohemorrhagic Escherichia coli (E...   \n",
       "3    The digestive tract is the entry site for tran...   \n",
       "4    BACKGROUND: FIV infection frequently compromis...   \n",
       "..                                                 ...   \n",
       "547  Abstract P/V gene substitutions convert the no...   \n",
       "548  Alcoholic liver disease (ALD) is characterized...   \n",
       "549  The unique ornamental features and extreme sex...   \n",
       "550  In the recent years, it has been demonstrated ...   \n",
       "551  We provide experimental evidence of a replicat...   \n",
       "\n",
       "                                         pattern_match  \\\n",
       "0    Therefore, we hypothesize that a MUC1 VNTR TAC...   \n",
       "1    We conducted a scoping review of the literatur...   \n",
       "2    Thus, we hypothesize that the expression of si...   \n",
       "3    Therefore, we hypothesized that RA could induc...   \n",
       "4    We hypothesized that FIV infection may cause d...   \n",
       "..                                                 ...   \n",
       "547  Here, we used two distinct animal model system...   \n",
       "548  We hypothesized that TREM‐1 signaling contribu...   \n",
       "549  Innate and adaptive immune genes involved in c...   \n",
       "550  Based on an extensive bibliography where the i...   \n",
       "551  We hypothesize that this modulatory role may b...   \n",
       "\n",
       "                                    merged_noun_chunks  \\\n",
       "0    [Therefore, ,, we, hypothesize, that, a_MUC1_V...   \n",
       "1    [We, conducted, a_scoping_review, of, the_lite...   \n",
       "2    [Thus, ,, we, hypothesize, that, the_expressio...   \n",
       "3    [Therefore, ,, we, hypothesized, that, RA, cou...   \n",
       "4    [We, hypothesized, that, FIV_infection, may, c...   \n",
       "..                                                 ...   \n",
       "547  [Here, ,, we, used, two_distinct_animal_model_...   \n",
       "548  [We, hypothesized, that, TREM‐1, signaling, co...   \n",
       "549  [Innate_and_adaptive_immune_genes, involved, i...   \n",
       "550  [Based, on, an_extensive_bibliography, where, ...   \n",
       "551  [We, hypothesize, that, this_modulatory_role, ...   \n",
       "\n",
       "                                           merged_sent  \n",
       "0    Therefore , we hypothesize that a_MUC1_VNTR_TA...  \n",
       "1    We conducted a_scoping_review of the_literatur...  \n",
       "2    Thus , we hypothesize that the_expression of s...  \n",
       "3    Therefore , we hypothesized that RA could indu...  \n",
       "4    We hypothesized that FIV_infection may cause d...  \n",
       "..                                                 ...  \n",
       "547  Here , we used two_distinct_animal_model_syste...  \n",
       "548  We hypothesized that TREM‐1 signaling contribu...  \n",
       "549  Innate_and_adaptive_immune_genes involved in c...  \n",
       "550  Based on an_extensive_bibliography where the_i...  \n",
       "551  We hypothesize that this_modulatory_role may b...  \n",
       "\n",
       "[551 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"hyps_merged_sents.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get specific Noun Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyps = pd.read_csv('hyps_merged_sents.csv')\n",
    "sent2parse = hyps[\"merged_sent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution from https://stackoverflow.com/questions/59993683/how-can-i-get-spacy-to-stop-splitting-both-hyphenated-numbers-and-words-into-sep \n",
    "def custom_tokenizer(nlp):\n",
    "    \"\"\"\n",
    "    Function that keeps intra-hyphenated words as single tokens.\n",
    "    \"\"\"\n",
    "    inf = list(nlp.Defaults.infixes)               # Default infixes\n",
    "    inf.remove(r\"(?<=[0-9])[+\\-\\*^](?=[0-9-])\")    # Remove the generic op between numbers or between a number and a -\n",
    "    inf = tuple(inf)                               # Convert inf to tuple\n",
    "    infixes = inf + tuple([r\"(?<=[0-9])[+*^](?=[0-9-])\", r\"(?<=[0-9])-(?=-)\"])  # Add the removed rule after subtracting (?<=[0-9])-(?=[0-9]) pattern\n",
    "    infixes = [x for x in infixes if '-|–|—|--|---|——|~' not in x] # Remove - between letters rule\n",
    "    infix_re = compile_infix_regex(infixes)\n",
    "\n",
    "    return Tokenizer(nlp.vocab, prefix_search=nlp.tokenizer.prefix_search,\n",
    "                                suffix_search=nlp.tokenizer.suffix_search,\n",
    "                                infix_finditer=infix_re.finditer,\n",
    "                                token_match=nlp.tokenizer.token_match,\n",
    "                                rules=nlp.Defaults.tokenizer_exceptions)\n",
    "\n",
    "nlp.tokenizer = custom_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_keywords(hypothesis_sentence):\n",
    "#     keywords = []\n",
    "#     for hypothesis in series:\n",
    "#         hypothesis_keywords = []\n",
    "#         doc = nlp(hypothesis)\n",
    "#         for tok in doc:\n",
    "#             if tok.pos_ == \"PROPN\" or tok.pos_ == \"NOUN\":\n",
    "#                 hypothesis_keywords.append(tok.text)\n",
    "#         keywords.append(hypothesis_keywords)\n",
    "#     return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(hypothesis_sentence):\n",
    "    hypothesis_keywords = []\n",
    "    doc = nlp(hypothesis_sentence)\n",
    "    for tok in doc:\n",
    "        if tok.pos_ == \"PROPN\" or tok.pos_ == \"NOUN\":\n",
    "            hypothesis_keywords.append(tok.text)\n",
    "    return hypothesis_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyps['keywords'] = hyps['merged_sent'].apply(get_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyps.to_csv(\"abstract_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
